{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48938a84",
   "metadata": {},
   "source": [
    "# Expected Utility Risk Averse MDP\n",
    "\n",
    "### 0. Utility function\n",
    "Here we define utility function for a discrete random variable $X$ given risk measure $\\rho$ and risk averse parameter $\\lambda$ as $U_\\rho^\\lambda(X)$. Utility function has a property (P0) $\\mathbb{E}[U_\\rho^\\lambda(X)] = \\rho_\\lambda[X]$. For certain risk measure (eg: $\\rho \\in \\{\\mathbb{E},\\text{VaR},\\text{CVaR},\\text{EVaR}\\}$), we can write $U_\\rho^\\lambda(x) = z(x) \\cdot x$ where $z$ is a function of $x$.\n",
    "\n",
    "- Expected value utility function\n",
    "    $$U_\\mathbb{E}(x) = x = 1 \\cdot x$$\n",
    "    Property (P0) follows trivially\n",
    "    $$\\mathbb{E}[U_\\mathbb{E}(X)] = \\mathbb{E}[X]$$\n",
    "\n",
    "- Value at risk utility function\n",
    "$$U_\\text{VaR}^\\lambda(x) = \n",
    "\\begin{cases}  \n",
    "\\frac{1}{\\mathbb{P}[X = x_\\lambda]} \\cdot x &, x = x_\\lambda\\\\\n",
    "0 &, \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "where $x_\\lambda = \\inf\\{x \\in \\mathbb{R} : P(X \\leq x) \\geq \\lambda \\} = \\text{VaR}_\\lambda[X]$ is the $\\lambda^\\text{th}$ percentile of $X$. \n",
    "Property (P0) follows:\n",
    "$$\\mathbb{E}[U_\\text{VaR}^\\lambda(x)] = \\mathbb{P}[X \\neq x_\\lambda] \\cdot 0  + \\mathbb{P}[X = x_\\lambda] \\cdot \\frac{x_\\lambda}{ \\mathbb{P}[X = x_\\lambda]} = x_\\lambda = \\text{VaR}_\\lambda[X]$$\n",
    "\n",
    "- Conditional Value at risk utility function\n",
    "$$U_\\text{CVaR}^\\lambda(x) = \n",
    "\\begin{cases}  \n",
    "\\frac{1}{\\lambda} \\cdot x &,  x < x_\\lambda\\\\\n",
    "\\frac{\\lambda ~- ~\\mathbb{P}[X < x_\\lambda]}{\\lambda} \\cdot x &,  x = x_\\lambda\\\\\n",
    "0 &, \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "Property (P0) Follow:\n",
    "$$\\mathbb{E}[U_\\text{CVaR}^\\lambda(x)] = \\frac{\\mathbb{E}[X~ 1_{\\{X < x_\\lambda\\}}]}{\\lambda} + x_\\lambda \\cdot \\frac{(\\lambda - \\mathbb{P}[X < x_\\lambda])}{\\lambda} = \\text{CVaR}_\\lambda[X]$$\n",
    "\n",
    "- Entropic Value at risk utility function\n",
    "$$U_\\text{EVaR}^\\lambda(x) = x \\cdot \\frac{e^{-\\beta^\\star x}}{\\mathbb{E}[e^{-\\beta^\\star X}]}$$\n",
    "where $\\beta^\\star = \\text{argmax}_\\beta\\{ -\\beta^{-1} \\cdot [\\log(\\mathbb{E}[e^{-\\beta X}])-\\log(\\lambda)] \\}$. Furthermore, let $Z^\\star = \\frac{e^{-\\beta^\\star X}}{\\mathbb{E}[e^{-\\beta^\\star X}]}$ we have $\\mathbb{E}[Z^\\star] = 1$ and $\\mathbb{E}[Z^\\star \\log(Z^\\star)] = -\\log(\\lambda)$. Property (P0) follows from the dual of EVaR:\n",
    "$$\\mathbb{E}[U_\\text{EVaR}^\\lambda(x)] = \\mathbb{E}[X \\cdot \\frac{ e^{-\\beta^\\star X}}{\\mathbb{E}[e^{-\\beta^\\star X}]}] = \n",
    "\\mathbb{E}[X \\cdot Z^\\star]\n",
    "=\\inf_{Z > 0}\\{\\mathbb{E}[XZ] : \\mathbb{E}[Z]=1,\\mathbb{E}[Z\\log(Z)]\\leq -\\log(\\lambda)\\}$$\n",
    "\n",
    "\n",
    "### 1. Distortion function\n",
    "Instead of changing the utility of the value in distribution $X$. Distortion function refer to the cumulative distribution of the (dual) robust distorted distribution $Q^\\star$. Where $Q^\\star$ is defined as\n",
    "\n",
    "$$\n",
    "\\rho_\\lambda[X] = \\mathbb{E}_{Q^\\star}[X] = \\inf_{Q \\in \\mathcal{Q}}(~\\mathbb{E}_Q[X]~)\n",
    "$$\n",
    "\n",
    "Instead of distorted (cumulative distribution) we first make a connection of utility function with distorted probability mass function (PMF). Here, we connect expected utility and the probability mass function (PMF) of the robust distribution $Q^\\star$ with respect to certain risk measure (eg: $\\rho \\in \\{\\mathbb{E},\\text{VaR},\\text{CVaR},\\text{EVaR}\\}$) where $\\rho_\\lambda[X] = \\mathbb{E}[U_\\rho^\\lambda(X)] = \\mathbb{E}[z(X) \\cdot X]$  over random variable $X$.\n",
    "\n",
    "$$\n",
    "\\rho_\\lambda[X] = \\mathbb{E}[U_\\rho^\\lambda(X)] = \\sum_x p(x)\\cdot U^\\lambda\\rho(X) = \\sum_x p(x) \\cdot z(x) \\cdot x = \\sum_x q(x) \\cdot x  = \\mathbb{E}_{Q^\\star}[X]\n",
    "$$\n",
    "\n",
    "Now we are ready to defined the Distortion (cumulative distribution) function $D_\\rho^\\lambda(~ F^P_X(x) ~) = F^{Q^\\star}_X(x)$. Given risk measure $\\rho$ and level $\\lambda$, the distortion function take in the CDF of the original distribution $P$ of $X$ and output the CDF of the robust distorted distribution $Q^\\star$ of $X$ .\n",
    "\n",
    "- Expected value distortion function\n",
    "$$\n",
    "\\text{D}_\\mathbb{E}( ~F^P_X(x)~ ) = F^{Q^\\star}_X(x) = F^P_X(x)\n",
    "$$\n",
    "\n",
    "- Value at risk distortion function\n",
    "$$\n",
    "\\text{D}_\\text{var}^\\lambda( ~F^P_X(x)~) = F^{Q^\\star}_X(x) = \n",
    "\\begin{cases}  \n",
    "0 &, 0 \\leq F^P_X(x) < \\lambda\\\\\n",
    "1 &, \\lambda \\leq F^P_X(x) \\leq 1\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "- Conditional value at risk distortion function\n",
    "$$\n",
    "\\text{D}_\\text{cvar}^\\lambda(~F^P_X(x)~) = F^{Q^\\star}_X(x) = \n",
    "\\begin{cases}  \n",
    "\\frac{F^P_X(x)}{\\lambda} &, 0 \\leq F^P_X(x) < \\lambda\\\\\n",
    "1 &, \\lambda \\leq F^P_X(x) \\leq 1\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "- Entropic value at risk distortion function\n",
    "$$\n",
    "\\text{D}_\\text{evar}^\\lambda(~F^P_X(x)~) = F^{Q^\\star}_X(x) = \\sum_{\\{\\chi \\leq x~:~\\chi \\in X\\}} \\big( ~p(\\chi) \\cdot \\frac{e^{-\\beta^\\star \\chi}}{\\mathbb{E}[e^{-\\beta^\\star X}]} ~\\big) ~\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3cd029",
   "metadata": {},
   "source": [
    "### 2. Standard MDP\n",
    "A standard MDP is defined as $(S,A,P,R,\\gamma,T)$. Where state $s \\in S$, action $a \\in A$, transition probability $P(s,a) = \\triangle^S$, reward $r(s,a,s') \\in R \\subset \\mathbb{R} \\backslash \\{ - \\infty , \\infty \\}$, discount factor $\\gamma \\in (0,1)$, and time horizon $T \\in \\mathbb{Z}^{+}$. A standard MDP is optimizes with the objective that maximizes the expected total discounted reward as\n",
    "$$\\max_{\\pi \\in \\Pi} \\mathbb{E}\\big[ \\sum^T_{t=0} \\gamma^t R^\\pi(s_t,a_t,s_{t+1}) \\big] = \\max_{\\pi \\in \\Pi} \\mathbb{E}\\big[ \\mathfrak{R}_{T}(\\pi) \\big]$$\n",
    "where $\\mathfrak{R}_{T}(\\pi) = \\sum^T_{t=0} \\gamma^t R^\\pi(s_t,a_t,s_{t+1})$ denote the random variable total discounted reward. \n",
    "\n",
    "The optimal policy for standard MDP can be computed with Linear Program (Value Iteration, Policy Iteration and Linear Programming). As for Value Iteration, we have the greedy bellman operator\n",
    "$$\\mathfrak{T}v(s) = \\max_{a \\in A} \\sum_{s' \\in S}\\big[ p(s,a,s') \\cdot [r(s,a,s') + \\gamma \\cdot v(s')] \\big].$$\n",
    "The optimal policy achieved by this bellman operator is Markovian Deterministic for infinite horizon $T = \\infty$ and Time Dependent Deterministic for finite horizon $T<\\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4f5b76",
   "metadata": {},
   "source": [
    "### 3. Expected Utility Risk Averse MDP\n",
    "\n",
    "Unlike Standard MDP, Risk Averse MDP repalce the risk neutral expectation with risk measure $\\rho$ as \n",
    "$$\\max_{\\pi \\in \\Pi} \\rho \\big[ \\sum^T_{t=0} \\gamma^t R^\\pi(s_t,a_t,s_{t+1}) \\big] = \\max_{\\pi \\in \\Pi} \\rho \\big[ \\mathfrak{R}_{T}(\\pi) \\big]$$\n",
    "\n",
    "The Risk neutral expectation can be computed efficiently because it consist of three desireble properties : (P1) Convexity, (P2) Time Consistency, (P3) Positive Homogeneity. Despite a small change in the objective function, the problem is much harder to solve due to the new risk measure $\\rho$ unable to satisfy all (P1,P2,P3). \n",
    "\n",
    "In expected utility Risk Averse MDP, instead of using risk level of a metric, we aim to use the target value instead. For VaR and CVaR the target value is defined as $x_\\lambda = \\text{VaR}_\\lambda[X]$. Instead of fixing $\\lambda$ and calculate for $x_\\lambda$, we fixed $x_\\lambda$ and solve for the problem where the target value is optimum $x_\\lambda$. The risk averse parameter $\\lambda$ and the policy $\\pi$ will be return as solution to our algorithm.\n",
    "\n",
    "##### 3.1 Algorithm (EUVAR)\n",
    "- Input: MDP $(S,A,P,R,\\gamma,T)$, precision $p$ and target value $x_\\lambda$.\n",
    "- Output: $\\pi \\in \\Pi$ optimal policy and $\\lambda \\in [0,1]$ risk level the target value optimizes.\n",
    "- Algorithm: \n",
    "    - Construct MDP with augment state-space.\n",
    "    - Solve the optimal policy $\\pi$ of this MDP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d18c551",
   "metadata": {},
   "outputs": [],
   "source": [
    "using DataFrames\n",
    "using CSV \n",
    "using StatsBase\n",
    "using Plots\n",
    "using SparseArrays \n",
    "using ProgressBars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f374f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a data frame to MDP.\n",
    "function df2MDP(df;γ = 0.90)\n",
    "    S = unique([df.idstatefrom;df.idstateto])\n",
    "    A = unique(df.idaction)\n",
    "    lSl = length(S)\n",
    "    lAl = length(A)\n",
    "    P = zeros((lSl,lAl,lSl))\n",
    "    R = zeros((lSl,lAl,lSl))\n",
    "    for i in eachrow(df)\n",
    "        P[i.idstatefrom,i.idaction,i.idstateto] += i.probability\n",
    "        R[i.idstatefrom,i.idaction,i.idstateto] += i.reward\n",
    "    end\n",
    "    return (S=S,A=A,P=P,R=R,lSl=lSl,lAl=lAl,γ=γ)\n",
    "end\n",
    "\n",
    "df = CSV.read(\"C:/GITHUB/rmdp-jl-2/data/TabMDP/riverswim.csv\", DataFrame)\n",
    "# The document uses \"zero index\" so we need to change to \"one index for julia\"\n",
    "df[:,[\"idstatefrom\",\"idaction\",\"idstateto\"]] = df[:,[\"idstatefrom\",\"idaction\",\"idstateto\"]] .+ 1\n",
    "mdpO = df2MDP(df;γ = 0.9);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e6385d",
   "metadata": {},
   "outputs": [],
   "source": [
    "function solveStandard(mdp;T=1000)\n",
    "    vnew = zeros((1,1,mdp.lSl))\n",
    "    v = ones((1,1,1))\n",
    "    FM = 0\n",
    "    for t in ProgressBar(1:T)\n",
    "        v = repeat(vnew,outer = (mdp.lSl,mdp.lAl,1))\n",
    "        q = sum( mdp.P .* (mdp.R .+ (mdp.γ .* v) ) ,dims=3)[:,:,1]\n",
    "        FM = findmax(q,dims = 2)\n",
    "        vnew[1,1,:] = FM[1][:,1]\n",
    "        if maximum(abs.( vnew[1,1,:] .- v[1,1,:] )) < 1e-13\n",
    "            println(\"iteration \",t)\n",
    "            return(v=vnew[1,1,:],pi = [fm[2] for fm in FM[2][:,1]])\n",
    "        end\n",
    "    end\n",
    "    return(v=vnew[1,1,:],pi = [fm[2] for fm in FM[2][:,1]])\n",
    "end\n",
    "time = @elapsed soln = solveStandard(mdpO,T=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7135b828",
   "metadata": {},
   "outputs": [],
   "source": [
    "function eval(pi,MDP;T=1000,episodes = 10000, s0 = sample(MDP.S, Weights(ones(MDP.lSl) ./ MDP.lSl) , episodes) )\n",
    "    rewards = zeros(episodes,T)\n",
    "    for i in 1:episodes\n",
    "        s2 = s0[i]\n",
    "        for t in 1:T\n",
    "            s = s2\n",
    "            a = pi[s]\n",
    "            s2 = sample(MDP.S, Weights(MDP.P[s,a,:]), 1)[1]\n",
    "            rewards[i,t] = MDP.R[s,a,s2]\n",
    "        end\n",
    "    end\n",
    "    return rewards\n",
    "end\n",
    "function cumulative(v,γ,T)   \n",
    "    return v * ( γ .^ (0:T-1) )\n",
    "end\n",
    "T = 10000\n",
    "sims = eval(soln.pi,mdpO;T = T)\n",
    "cumrets = cumulative(sims, mdpO.γ,T);\n",
    "Qval = quantile(cumrets,collect(0:0.01:1))\n",
    "xopt = quantile(cumrets,0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9402f3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram(cumrets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50827134",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "function target_agg(R,γ)\n",
    "    m = round(Int,minimum(R)/(1-γ))\n",
    "    M = round(Int,maximum(R)/(1-γ))\n",
    "    X = round.(Int,m:1:M)\n",
    "    return X    \n",
    "end\n",
    "function Rscale( R;digits = 3)\n",
    "    Rnew = round.( Int, (R) * (10^digits) )\n",
    "    return Rnew\n",
    "end\n",
    "function AugmentVaR(mdp,xopt;digits = 0)\n",
    "    scaledR = Rscale(mdp.R,digits = digits)\n",
    "    xopt = Rscale(xopt,digits = digits)\n",
    "    X = target_agg(scaledR,mdp.γ)\n",
    "    lXl = length(X)\n",
    "    Xmax = last(X)\n",
    "    Xmin = X[1]\n",
    "\n",
    "    XInS =  reduce(vcat, ones(Int,size(mdp.S')) .* X)\n",
    "    S = reduce(vcat, string.(mdp.S') .* \"-\" .* string.(X))\n",
    "    Smap = Dict(s => i for (i,s) in enumerate(S))\n",
    "    A = mdp.A\n",
    "    lSl = length(S)\n",
    "    lAl = length(A)\n",
    "    R = zeros(lSl,lAl,lSl)\n",
    "    Rtemp = ((XInS .< xopt) .* (XInS .>= xopt)') .- ((XInS .< xopt)' .* (XInS .>= xopt))\n",
    "    for a in A\n",
    "        R[:,a,:] = Rtemp\n",
    "    end\n",
    "    P = zeros(lSl,lAl,lSl)\n",
    "    for s in 1:mdp.lSl\n",
    "        for a in 1:mdp.lAl\n",
    "            for s2 in 1:mdp.lSl\n",
    "                # We use floor here to underestimate the value\n",
    "                X2 = floor.(Int,min.(Xmax,(mdp.γ * X .+ scaledR[s,a,s2])))\n",
    "                s_hat1 = map(x -> Smap[x], (string(s).*\"-\".*string.(X)) ) \n",
    "                s_hat2 = map(x -> Smap[x], (string(s2).*\"-\".*string.(X2)) ) \n",
    "                Index = [CartesianIndex.(v1,a,v2) for (v1, v2) in zip(s_hat1, s_hat2)]\n",
    "                P[Index] .= mdp.P[s,a,s2]\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return(S=S,A=A,X=X,lSl=lSl,lAl=lAl,lXl=lXl,Smap=Smap,\n",
    "        P=P,R=R,γ=1,digits = digits,Rmin = minimum(mdp.R) )\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee69993",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mdp = AugmentVaR(mdpO,xopt;digits =0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dfd2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112ca059",
   "metadata": {},
   "outputs": [],
   "source": [
    "time = @elapsed soln2 = solveStandard(mdp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8915bb5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "map(x -> soln2.v[mdp.Smap[x]], (reduce(vcat, string.(mdpO.S) .* \"-\" .*string(mdp.xopt))) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404ef901",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "map(x -> soln2.pi[mdp.Smap[x]], (reduce(vcat, string.(mdpO.S) .* \"-\" .*string(mdp.xopt))) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086b3399",
   "metadata": {},
   "outputs": [],
   "source": [
    "function evalAugmentReward(pi,AugMDP,OriMDP;T=1000,episodes = 10000,xopt= AugMDP.xopt,\n",
    "        s0 = sample(OriMDP.S, Weights(ones(OriMDP.lSl) ./ OriMDP.lSl) , episodes) )\n",
    "    rewards = zeros(episodes)\n",
    "    for i in 1:episodes\n",
    "        s2 = s0[i]\n",
    "        for t in 1:T\n",
    "            s = s2\n",
    "            x = Rscale(rewards[i]+xopt,digits = AugMDP.digits)\n",
    "            augmented_s = AugMDP.Smap[string(s)*\"-\"*string(x)]\n",
    "            a = pi[augmented_s]\n",
    "            s2 = sample(OriMDP.S, Weights(OriMDP.P[s,a,:]), 1)[1]\n",
    "            rewards[i] += ((OriMDP.γ^t) * OriMDP.R[s,a,s2])\n",
    "        end\n",
    "    end\n",
    "    return rewards\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be65326",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cumretsVaR = evalAugmentReward(soln2.pi,mdp,mdpO,xopt=xopt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0f142e",
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram(cumretsVaR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8fe325",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xoptNew = quantile(cumretsVaR,0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9106701",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a3963a",
   "metadata": {},
   "outputs": [],
   "source": [
    "squarePi = reshape(soln2.pi,(mdpO.lSl,mdp.lXl))\n",
    "heatmap(1:mdpO.lSl,1:mdp.lXl, squarePi,c=([:blue,:red]),\n",
    "xlabel=\"States\", ylabel=\"Cumulative Reward\", title=\"Augmented Return VaR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f32b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaledR = Rscale(mdpO.R,digits = 0)\n",
    "X = target_agg(scaledR,mdpO.γ)\n",
    "lXl = length(X)\n",
    "Xmax = last(X)\n",
    "Xmin = X[1]\n",
    "\n",
    "XInS =  reduce(vcat, ones(Int,size(mdpO.S')) .* X)\n",
    "SInS =  reduce(vcat, mdpO.S' .* ones(Int,size(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f20eaf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "XInS[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc775987",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rtemp = zeros(mdp.lSl,mdp.lAl,mdp.lSl);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804f5056",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xbeq = findall(XInS .>= xopt)\n",
    "Xl = findall(XInS .< xopt)\n",
    "for i in 1:mdp.lSl\n",
    "    if XInS[i] < xopt\n",
    "        for a in mdp.A\n",
    "            Rtemp[i,a,Xbeq] .= 1\n",
    "        end\n",
    "    else\n",
    "        for a in mdp.A\n",
    "            Rtemp[i,a,Xl] .= -1\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b90552",
   "metadata": {},
   "outputs": [],
   "source": [
    "maximum(abs.(Rtemp .- mdp.R))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2978cf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb22ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "S = collect(1:1000)\n",
    "A = collect(1:1000)\n",
    "lSl = length(S)\n",
    "lAl = length(A)\n",
    "R = repeat(reshape([repeat([0],999);1],(1,1,lSl)),outer=(lSl,lAl,1))\n",
    "P = zeros((length(S),length(A),length(S)))\n",
    "ph = 0.4\n",
    "\n",
    "for s in S\n",
    "    for a in A\n",
    "        if (s>=a)\n",
    "            if (s+a <= 1000)\n",
    "                P[s,a,s+a]=ph\n",
    "                if (s-a > 0)\n",
    "                  P[s,a,s-a]=1-ph\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "end\n",
    "MDP = (S=S,A=A,P=P,lSl=lSl,lAl=lAl,R=R,γ=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d1e662",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gamblersSoln = solveStandard(MDP)\n",
    "plot(MDP.S,gamblersSoln.pi,linetype=:steppre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda4d8de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.5",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
